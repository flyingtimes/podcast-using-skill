# ğŸ“‹ Crawl æŠ€èƒ½æœ€ä½³å®è·µæŒ‡å—

## ğŸ¯ ç›®å½•

| å®è·µ | æè¿° | é‡è¦æ€§ |
|------|------|--------|
| [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®æœ€ä½³å®è·µ) | å¼€å‘ç¯å¢ƒè®¾ç½® | â­â­â­â­â­ |
| [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–æŒ‡å—) | æå‡æ‰§è¡Œæ•ˆç‡ | â­â­â­â­ |
| [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†ç­–ç•¥) | å¥å£®æ€§ä¿éšœ | â­â­â­â­â­ |
| [æ•°æ®è´¨é‡](#æ•°æ®è´¨é‡ç®¡ç†) | ç¡®ä¿æ•°æ®å‡†ç¡®æ€§ | â­â­â­â­ |
| [å®‰å…¨åˆè§„](#å®‰å…¨ä¸åˆè§„) | åˆæ³•åˆè§„ä½¿ç”¨ | â­â­â­â­â­ |

---

## ğŸ”§ ç¯å¢ƒé…ç½®æœ€ä½³å®è·µ

### 1. ç³»ç»Ÿè¦æ±‚æ£€æŸ¥
```python
def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    import platform
    import subprocess
    import os

    requirements = {
        "python_version": "3.8+",
        "chrome_installed": False,
        "uv_installed": False,
        "disk_space": "1GB+"
    }

    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = platform.python_version()
    print(f"Pythonç‰ˆæœ¬: {python_version}")

    # æ£€æŸ¥Chromeå®‰è£…
    system = platform.system()
    if system == "Windows":
        chrome_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
        ]
        for path in chrome_paths:
            if os.path.exists(path):
                requirements["chrome_installed"] = True
                break
    elif system == "Darwin":
        if os.path.exists("/Applications/Google Chrome.app"):
            requirements["chrome_installed"] = True

    # æ£€æŸ¥uvå®‰è£…
    try:
        subprocess.run(["uv", "--version"], capture_output=True, check=True)
        requirements["uv_installed"] = True
    except:
        pass

    return requirements

# ä½¿ç”¨ç¤ºä¾‹
requirements = check_system_requirements()
print("ç³»ç»Ÿè¦æ±‚æ£€æŸ¥ç»“æœ:", requirements)
```

### 2. é…ç½®æ–‡ä»¶ç®¡ç†
```python
# config.py - æ¨èçš„é…ç½®ç»“æ„
DEFAULT_CONFIG = {
    # Chromeé…ç½®
    "chrome": {
        "port": 9222,
        "proxy_url": "http://127.0.0.1:1087",
        "user_data_dir": "./userdata",
        "headless": False,
        "window_size": "1920,1080"
    },

    # APIé…ç½®
    "api": {
        "base_url": "http://localhost:8000",
        "timeout": 30,
        "max_retries": 3,
        "retry_delay": 5
    },

    # è¾“å‡ºé…ç½®
    "output": {
        "base_dir": "output",
        "log_level": "INFO",
        "save_snapshots": True,
        "compress_data": False
    },

    # çˆ¬å–é…ç½®
    "crawl": {
        "default_count": 5,
        "request_delay": 2,
        "page_load_timeout": 15,
        "element_wait_timeout": 10
    }
}

def load_config(custom_config: dict = None):
    """åŠ è½½é…ç½®"""
    config = DEFAULT_CONFIG.copy()

    if custom_config:
        # æ·±åº¦åˆå¹¶é…ç½®
        for section, values in custom_config.items():
            if section in config:
                config[section].update(values)
            else:
                config[section] = values

    return config
```

### 3. ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env æ–‡ä»¶ç¤ºä¾‹
CHROME_PORT=9222
PROXY_URL=http://127.0.0.1:1087
API_BASE_URL=http://localhost:8000
LOG_LEVEL=INFO
OUTPUT_DIR=output
```

```python
# ç¯å¢ƒå˜é‡åŠ è½½
import os
from dotenv import load_dotenv

load_dotenv()

ENV_CONFIG = {
    "chrome": {
        "port": int(os.getenv("CHROME_PORT", 9222)),
        "proxy_url": os.getenv("PROXY_URL", "http://127.0.0.1:1087")
    },
    "api": {
        "base_url": os.getenv("API_BASE_URL", "http://localhost:8000"),
        "timeout": int(os.getenv("API_TIMEOUT", 30))
    }
}
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŒ‡å—

### 1. å¹¶å‘å¤„ç†
```python
import concurrent.futures
import threading
from queue import Queue

class ConcurrentCrawler:
    """å¹¶å‘çˆ¬è™«"""

    def __init__(self, max_workers: int = 3):
        self.max_workers = max_workers
        self.results = []

    def crawl_user(self, username: str, count: int = 5):
        """çˆ¬å–å•ä¸ªç”¨æˆ·"""
        try:
            result = extract_x_tweets(username, count)
            return {"username": username, "result": result}
        except Exception as e:
            return {"username": username, "error": str(e)}

    def crawl_multiple_users(self, usernames: list, count: int = 5):
        """å¹¶å‘çˆ¬å–å¤šä¸ªç”¨æˆ·"""
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = [
                executor.submit(self.crawl_user, username, count)
                for username in usernames
            ]

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                self.results.append(result)

        return self.results

# ä½¿ç”¨ç¤ºä¾‹
crawler = ConcurrentCrawler(max_workers=3)
usernames = ["elonmusk", "sundarpichai", "satyanadella", "tim_cook", "satyanadella"]
results = crawler.crawl_multiple_users(usernames, 3)

for result in results:
    if "error" not in result:
        print(f"{result['username']}: {result['result'].get('tweets_extracted', 0)} æ¡æ¨æ–‡")
    else:
        print(f"{result['username']}: çˆ¬å–å¤±è´¥ - {result['error']}")
```

### 2. ç¼“å­˜æœºåˆ¶
```python
import pickle
import hashlib
from datetime import datetime, timedelta

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "output/cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.cache_duration = timedelta(hours=1)  # ç¼“å­˜1å°æ—¶

    def _get_cache_key(self, username: str, count: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key = f"{username}_{count}_{datetime.now().strftime('%Y%m%d_%H')}"
        return hashlib.md5(key.encode()).hexdigest()

    def _is_cache_valid(self, cache_file: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_file):
            return False

        file_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
        return datetime.now() - file_time < self.cache_duration

    def get_cached_result(self, username: str, count: int):
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._get_cache_key(username, count)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")

        if self._is_cache_valid(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)

        return None

    def save_result(self, username: str, count: int, result):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        cache_key = self._get_cache_key(username, count)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")

        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)

# ä½¿ç”¨ç¤ºä¾‹
def extract_with_cache(username: str, count: int = 5):
    """å¸¦ç¼“å­˜çš„æ¨æ–‡æå–"""
    cache = CacheManager()

    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = cache.get_cached_result(username, count)
    if cached_result:
        print(f"ä»ç¼“å­˜è·å– {username} çš„æ¨æ–‡")
        return cached_result

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œçˆ¬å–
    print(f"çˆ¬å– {username} çš„æ¨æ–‡...")
    result = extract_x_tweets(username, count)

    # ä¿å­˜åˆ°ç¼“å­˜
    cache.save_result(username, count, result)

    return result
```

### 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
def batch_process_optimized(items: list, batch_size: int = 10, delay: float = 1.0):
    """ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†"""
    results = []
    total_items = len(items)

    for i in range(0, total_items, batch_size):
        batch = items[i:i + batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(total_items-1)//batch_size + 1}")

        batch_results = []
        for item in batch:
            try:
                result = extract_x_tweets(item, 3)
                batch_results.append({"item": item, "result": result})
            except Exception as e:
                batch_results.append({"item": item, "error": str(e)})

        results.extend(batch_results)

        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if i + batch_size < total_items:
            time.sleep(delay)

    return results
```

---

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†ç­–ç•¥

### 1. åˆ†å±‚é”™è¯¯å¤„ç†
```python
class CrawlError(Exception):
    """çˆ¬è™«åŸºç¡€å¼‚å¸¸"""
    pass

class ChromeError(CrawlError):
    """Chromeç›¸å…³é”™è¯¯"""
    pass

class APIError(CrawlError):
    """APIç›¸å…³é”™è¯¯"""
    pass

class DataValidationError(CrawlError):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass

def robust_extract_tweets(username: str, count: int = 5, max_retries: int = 3):
    """å¥å£®çš„æ¨æ–‡æå–"""
    for attempt in range(max_retries):
        try:
            # 1. ç¯å¢ƒæ£€æŸ¥
            if not check_environment():
                raise ChromeError("ç¯å¢ƒæ£€æŸ¥å¤±è´¥")

            # 2. æ•°æ®æå–
            result = extract_x_tweets(username, count)

            # 3. æ•°æ®éªŒè¯
            if not validate_result(result):
                raise DataValidationError("æ•°æ®éªŒè¯å¤±è´¥")

            return result

        except ChromeError as e:
            print(f"Chromeé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                restart_chrome()
                time.sleep(5)

        except APIError as e:
            print(f"APIé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                restart_api_service()
                time.sleep(10)

        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(10)

    raise CrawlError(f"ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")

def validate_result(result: dict) -> bool:
    """éªŒè¯ç»“æœæœ‰æ•ˆæ€§"""
    if not isinstance(result, dict):
        return False

    if 'success' not in result:
        return False

    if result.get('success') and 'tweets_extracted' in result:
        return result['tweets_extracted'] > 0

    return False
```

### 2. ç›‘æ§å’Œå‘Šè­¦
```python
import logging
from dataclasses import dataclass
from typing import List

@dataclass
class CrawlMetrics:
    """çˆ¬è™«æŒ‡æ ‡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_tweets: int = 0
    average_response_time: float = 0.0
    error_rate: float = 0.0

class CrawlMonitor:
    """çˆ¬è™«ç›‘æ§"""

    def __init__(self):
        self.metrics = CrawlMetrics()
        self.response_times = []

    def record_request(self, success: bool, response_time: float, tweets_count: int = 0):
        """è®°å½•è¯·æ±‚"""
        self.metrics.total_requests += 1
        self.response_times.append(response_time)

        if success:
            self.metrics.successful_requests += 1
            self.metrics.total_tweets += tweets_count
        else:
            self.metrics.failed_requests += 1

        # æ›´æ–°æŒ‡æ ‡
        self.metrics.error_rate = self.metrics.failed_requests / self.metrics.total_requests
        self.metrics.average_response_time = sum(self.response_times) / len(self.response_times)

    def check_health(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        if self.metrics.error_rate > 0.5:  # é”™è¯¯ç‡è¶…è¿‡50%
            return False

        if self.metrics.average_response_time > 60:  # å¹³å‡å“åº”æ—¶é—´è¶…è¿‡60ç§’
            return False

        return True

    def get_status_report(self) -> str:
        """è·å–çŠ¶æ€æŠ¥å‘Š"""
        return f"""
çˆ¬è™«çŠ¶æ€æŠ¥å‘Š:
- æ€»è¯·æ±‚æ•°: {self.metrics.total_requests}
- æˆåŠŸè¯·æ±‚æ•°: {self.metrics.successful_requests}
- å¤±è´¥è¯·æ±‚æ•°: {self.metrics.failed_requests}
- é”™è¯¯ç‡: {self.metrics.error_rate:.2%}
- å¹³å‡å“åº”æ—¶é—´: {self.metrics.average_response_time:.2f}ç§’
- æ€»æ¨æ–‡æ•°: {self.metrics.total_tweets}
- å¥åº·çŠ¶æ€: {'æ­£å¸¸' if self.check_health() else 'å¼‚å¸¸'}
"""

# å…¨å±€ç›‘æ§å™¨
monitor = CrawlMonitor()

def monitored_extract_tweets(username: str, count: int = 5):
    """å¸¦ç›‘æ§çš„æ¨æ–‡æå–"""
    start_time = time.time()

    try:
        result = extract_x_tweets(username, count)
        response_time = time.time() - start_time

        success = result.get('success', False)
        tweets_count = result.get('tweets_extracted', 0)

        monitor.record_request(success, response_time, tweets_count)

        # å¥åº·æ£€æŸ¥å‘Šè­¦
        if not monitor.check_health():
            logging.warning("çˆ¬è™«å¥åº·çŠ¶æ€å¼‚å¸¸")

        return result

    except Exception as e:
        response_time = time.time() - start_time
        monitor.record_request(False, response_time)

        logging.error(f"æ¨æ–‡æå–å¤±è´¥: {e}")
        raise
```

---

## ğŸ“Š æ•°æ®è´¨é‡ç®¡ç†

### 1. æ•°æ®éªŒè¯æ¡†æ¶
```python
from pydantic import BaseModel, validator
from typing import Optional, List

class TweetData(BaseModel):
    """æ¨æ–‡æ•°æ®æ¨¡å‹"""
    title: str
    subtitle: Optional[str] = None
    author: str
    url: str
    content: str
    entry_time: str

    @validator('title')
    def validate_title(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('æ ‡é¢˜ä¸èƒ½ä¸ºç©º')
        if len(v) > 500:
            raise ValueError('æ ‡é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡500å­—ç¬¦')
        return v.strip()

    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´')
        return v

    @validator('content')
    def validate_content(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('å†…å®¹ä¸èƒ½ä¸ºç©º')
        if len(v) < 10:
            raise ValueError('å†…å®¹é•¿åº¦ä¸èƒ½å°‘äº10å­—ç¬¦')
        return v.strip()

class CrawlResult(BaseModel):
    """çˆ¬å–ç»“æœæ¨¡å‹"""
    success: bool
    username: str
    tweets_extracted: int
    tweets_written: int
    files: dict
    api_response: dict
    message: str

def validate_tweet_data(data: dict) -> TweetData:
    """éªŒè¯æ¨æ–‡æ•°æ®"""
    try:
        return TweetData(**data)
    except Exception as e:
        raise DataValidationError(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")

def validate_api_response(response: dict) -> bool:
    """éªŒè¯APIå“åº”"""
    required_fields = ['success_count', 'skipped_count', 'successful_titles']

    for field in required_fields:
        if field not in response:
            return False

    return response['success_count'] >= 0
```

### 2. æ•°æ®æ¸…æ´—
```python
import re
from html import unescape

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""

    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        if not text:
            return ""

        # HTMLè§£ç 
        text = unescape(text)

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()\-"\']', '', text)

        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()

        return text

    @staticmethod
    def clean_url(url: str) -> str:
        """æ¸…æ´—URL"""
        if not url:
            return ""

        # ç§»é™¤æŸ¥è¯¢å‚æ•°ä¸­çš„è¿½è¸ªä¿¡æ¯
        url = re.sub(r'[?&](utm_|ref|source|fbclid)=[^&]*', '', url)

        # ç§»é™¤æœ«å°¾çš„#
        url = url.split('#')[0]

        return url

    @staticmethod
    def extract_hashtags(text: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        return re.findall(r'#\w+', text)

    @staticmethod
    def extract_mentions(text: str) -> List[str]:
        """æå–æåŠ"""
        return re.findall(r'@\w+', text)

def clean_tweet_data(tweet_data: dict) -> dict:
    """æ¸…æ´—æ¨æ–‡æ•°æ®"""
    cleaner = DataCleaner()

    cleaned_data = tweet_data.copy()

    # æ¸…æ´—æ–‡æœ¬å­—æ®µ
    if 'title' in cleaned_data:
        cleaned_data['title'] = cleaner.clean_text(cleaned_data['title'])

    if 'content' in cleaned_data:
        cleaned_data['content'] = cleaner.clean_text(cleaned_data['content'])

        # æå–æ ‡ç­¾å’ŒæåŠ
        cleaned_data['hashtags'] = cleaner.extract_hashtags(cleaned_data['content'])
        cleaned_data['mentions'] = cleaner.extract_mentions(cleaned_data['content'])

    # æ¸…æ´—URL
    if 'url' in cleaned_data:
        cleaned_data['url'] = cleaner.clean_url(cleaned_data['url'])

    return cleaned_data
```

### 3. æ•°æ®å»é‡
```python
class DataDeduplicator:
    """æ•°æ®å»é‡å™¨"""

    def __init__(self):
        self.seen_urls = set()
        self.seen_titles = set()

    def is_duplicate_by_url(self, url: str) -> bool:
        """åŸºäºURLåˆ¤æ–­é‡å¤"""
        return url in self.seen_urls

    def is_duplicate_by_title(self, title: str) -> bool:
        """åŸºäºæ ‡é¢˜åˆ¤æ–­é‡å¤"""
        return title in self.seen_titles

    def add_to_seen(self, url: str, title: str):
        """æ·»åŠ åˆ°å·²è§è®°å½•"""
        self.seen_urls.add(url)
        self.seen_titles.add(title)

    def filter_duplicates(self, items: List[dict]) -> List[dict]:
        """è¿‡æ»¤é‡å¤é¡¹"""
        filtered_items = []

        for item in items:
            url = item.get('url', '')
            title = item.get('title', '')

            if not self.is_duplicate_by_url(url) and not self.is_duplicate_by_title(title):
                filtered_items.append(item)
                self.add_to_seen(url, title)
            else:
                print(f"è·³è¿‡é‡å¤é¡¹: {title[:50]}...")

        return filtered_items

# ä½¿ç”¨ç¤ºä¾‹
deduplicator = DataDeduplicator()

# å‡è®¾è¿™æ˜¯ä»APIè·å–çš„æ•°æ®
raw_data = {"essays": [...]}

# è¿‡æ»¤é‡å¤é¡¹
unique_tweets = deduplicator.filter_duplicates(raw_data["essays"])
print(f"å»é‡å: {len(unique_tweets)} æ¡å”¯ä¸€æ¨æ–‡")
```

---

## ğŸ›¡ï¸ å®‰å…¨ä¸åˆè§„

### 1. è¯·æ±‚é¢‘ç‡æ§åˆ¶
```python
import time
import random
from threading import Lock

class RateLimiter:
    """è¯·æ±‚é¢‘ç‡é™åˆ¶å™¨"""

    def __init__(self, requests_per_second: float = 1.0):
        self.requests_per_second = requests_per_second
        self.min_interval = 1.0 / requests_per_second
        self.last_request_time = 0
        self.lock = Lock()

    def wait(self):
        """ç­‰å¾…ç›´åˆ°å¯ä»¥å‘èµ·ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time

            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                # æ·»åŠ éšæœºæŠ–åŠ¨
                sleep_time += random.uniform(0, sleep_time * 0.1)
                time.sleep(sleep_time)

            self.last_request_time = time.time()

# å…¨å±€é¢‘ç‡é™åˆ¶å™¨
rate_limiter = RateLimiter(requests_per_second=0.5)  # æ¯2ç§’æœ€å¤š1æ¬¡è¯·æ±‚

def rate_limited_extract_tweets(username: str, count: int = 5):
    """é¢‘ç‡é™åˆ¶çš„æ¨æ–‡æå–"""
    rate_limiter.wait()
    return extract_x_tweets(username, count)
```

### 2. User-Agentè½®æ¢
```python
class UserAgentRotator:
    """User-Agentè½®æ¢å™¨"""

    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        self.current_index = 0

    def get_user_agent(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªUser-Agent"""
        user_agent = self.user_agents[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.user_agents)
        return user_agent

user_agent_rotator = UserAgentRotator()
```

### 3. æ•°æ®éšç§ä¿æŠ¤
```python
import hashlib
import re

class PrivacyProtector:
    """éšç§ä¿æŠ¤å™¨"""

    @staticmethod
    def hash_sensitive_data(data: str) -> str:
        """å“ˆå¸Œæ•æ„Ÿæ•°æ®"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]

    @staticmethod
    def mask_email(text: str) -> str:
        """é®è”½é‚®ç®±åœ°å€"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        return re.sub(email_pattern, lambda m: m.group()[0] + '***@' + m.group().split('@')[1], text)

    @staticmethod
    def mask_phone(text: str) -> str:
        """é®è”½ç”µè¯å·ç """
        phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'
        return re.sub(phone_pattern, lambda m: m.group()[:3] + '-***-' + m.group()[-4:], text)

    @staticmethod
    def anonymize_content(content: str) -> str:
        """åŒ¿ååŒ–å†…å®¹"""
        content = PrivacyProtector.mask_email(content)
        content = PrivacyProtector.mask_phone(content)
        return content

def anonymize_tweet_data(tweet_data: dict) -> dict:
    """åŒ¿ååŒ–æ¨æ–‡æ•°æ®"""
    anonymized = tweet_data.copy()

    if 'content' in anonymized:
        anonymized['content'] = PrivacyProtector.anonymize_content(anonymized['content'])

    return anonymized
```

### 4. åˆè§„æ£€æŸ¥æ¸…å•
```python
def compliance_check(url: str, content: str) -> dict:
    """åˆè§„æ£€æŸ¥"""
    issues = []

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰ˆæƒä¿¡æ¯
    if 'Â©' in content or 'copyright' in content.lower():
        issues.append("åŒ…å«ç‰ˆæƒä¿¡æ¯")

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸ªäººéšç§ä¿¡æ¯
    if re.search(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', content):  # ä¿¡ç”¨å¡å·
        issues.append("å¯èƒ½åŒ…å«ä¿¡ç”¨å¡å·")

    if re.search(r'\b\d{3}-\d{2}-\d{4}\b', content):  # ç¤¾ä¼šä¿é™©å·
        issues.append("å¯èƒ½åŒ…å«ç¤¾ä¼šä¿é™©å·")

    # æ£€æŸ¥robots.txtï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    if 'private' in url.lower() or 'admin' in url.lower():
        issues.append("è®¿é—®ç§æœ‰é¡µé¢")

    return {
        "compliant": len(issues) == 0,
        "issues": issues
    }

def is_crawling_allowed(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦å…è®¸çˆ¬å–"""
    # ç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”è¯¥æ£€æŸ¥robots.txt
    blocked_domains = ['mail.google.com', 'facebook.com/messages']

    for domain in blocked_domains:
        if domain in url:
            return False

    return True
```

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

### 1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†
```python
import psutil
import time
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    cpu_percent: float
    memory_percent: float
    disk_usage: float
    network_io: Dict[str, int]
    execution_time: float
    success_rate: float

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = None
        self.metrics_history: List[PerformanceMetrics] = []

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()

    def collect_metrics(self, success_count: int, total_count: int) -> PerformanceMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        # ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        cpu_percent = psutil.cpu_percent()
        memory_percent = psutil.virtual_memory().percent
        disk_usage = psutil.disk_usage('/').percent

        # ç½‘ç»œIO
        network_io = psutil.net_io_counters()._asdict()

        # æ‰§è¡Œæ—¶é—´å’ŒæˆåŠŸç‡
        execution_time = time.time() - self.start_time if self.start_time else 0
        success_rate = success_count / total_count if total_count > 0 else 0

        metrics = PerformanceMetrics(
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            disk_usage=disk_usage,
            network_io=network_io,
            execution_time=execution_time,
            success_rate=success_rate
        )

        self.metrics_history.append(metrics)
        return metrics

    def get_performance_report(self) -> str:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics_history:
            return "æš‚æ— æ€§èƒ½æ•°æ®"

        latest = self.metrics_history[-1]
        avg_cpu = sum(m.cpu_percent for m in self.metrics_history) / len(self.metrics_history)
        avg_memory = sum(m.memory_percent for m in self.metrics_history) / len(self.metrics_history)

        return f"""
æ€§èƒ½æŠ¥å‘Š:
- CPUä½¿ç”¨ç‡: {latest.cpu_percent:.1f}% (å¹³å‡: {avg_cpu:.1f}%)
- å†…å­˜ä½¿ç”¨ç‡: {latest.memory_percent:.1f}% (å¹³å‡: {avg_memory:.1f}%)
- ç£ç›˜ä½¿ç”¨ç‡: {latest.disk_usage:.1f}%
- æ‰§è¡Œæ—¶é—´: {latest.execution_time:.2f}ç§’
- æˆåŠŸç‡: {latest.success_rate:.2%}
"""

# ä½¿ç”¨ç¤ºä¾‹
perf_monitor = PerformanceMonitor()
perf_monitor.start_monitoring()

# æ‰§è¡Œçˆ¬å–ä»»åŠ¡...
# result = extract_x_tweets("elonmusk", 5)

# æ”¶é›†æ€§èƒ½æŒ‡æ ‡
# metrics = perf_monitor.collect_metrics(
#     success_count=result.get('tweets_written', 0),
#     total_count=result.get('tweets_extracted', 0)
# )
# print(perf_monitor.get_performance_report())
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•
1. **ä½¿ç”¨æ”¹è¿›ç‰ˆç®¡ç†å™¨** - åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
2. **å¯ç”¨ç¼“å­˜æœºåˆ¶** - é¿å…é‡å¤è¯·æ±‚ï¼Œæé«˜æ•ˆç‡
3. **æ§åˆ¶è¯·æ±‚é¢‘ç‡** - é¿å…è¢«ç½‘ç«™é™åˆ¶
4. **éªŒè¯æ•°æ®æ ¼å¼** - ç¡®ä¿æ•°æ®è´¨é‡
5. **ç›‘æ§æ€§èƒ½æŒ‡æ ‡** - åŠæ—¶å‘ç°é—®é¢˜
6. **ä¿æŠ¤ç”¨æˆ·éšç§** - åŒ¿ååŒ–æ•æ„Ÿä¿¡æ¯
7. **éµå®ˆrobots.txt** - åˆæ³•åˆè§„çˆ¬å–

### âŒ é¿å…åšæ³•
1. **å¿½ç•¥é”™è¯¯å¤„ç†** - å¯¼è‡´ç¨‹åºå´©æºƒ
2. **è¿‡åº¦é¢‘ç¹è¯·æ±‚** - å¯èƒ½è¢«IPå°ç¦
3. **ä¿å­˜åŸå§‹æ•æ„Ÿæ•°æ®** - éšç§æ³„éœ²é£é™©
4. **å¿½ç•¥èµ„æºé™åˆ¶** - å¯èƒ½å¯¼è‡´ç³»ç»Ÿå´©æºƒ
5. **ä¸éªŒè¯æ•°æ®æ ¼å¼** - å¯¼è‡´APIå†™å…¥å¤±è´¥

### ğŸ“‹ æ£€æŸ¥æ¸…å•
åœ¨éƒ¨ç½²çˆ¬è™«ä»»åŠ¡å‰ï¼Œè¯·ç¡®è®¤ï¼š

- [ ] ç¯å¢ƒé…ç½®æ­£ç¡®
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] è¯·æ±‚é¢‘ç‡åˆç†
- [ ] æ•°æ®éªŒè¯æœ‰æ•ˆ
- [ ] æ—¥å¿—è®°å½•å®Œæ•´
- [ ] æ€§èƒ½ç›‘æ§å¯ç”¨
- [ ] éšç§ä¿æŠ¤åˆ°ä½
- [ ] åˆè§„æ£€æŸ¥é€šè¿‡

---

## ğŸ“š æ›´å¤šèµ„æº

- [å®Œæ•´ç¤ºä¾‹](EXAMPLES.md)
- [å¿«é€Ÿå¯åŠ¨](QUICK_START.md)
- [æ ¸å¿ƒç®¡ç†å™¨](crawl_manager.py)