# ğŸš€ Crawl æŠ€èƒ½å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸ“‹ å‰ç½®æ¡ä»¶æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹ä½¿ç”¨ crawl æŠ€èƒ½ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»¥ä¸‹æ¡ä»¶å·²æ»¡è¶³ï¼š

### âœ… ç¯å¢ƒæ£€æŸ¥
- [ ] Python 3.8+ å·²å®‰è£…
- [ ] uv åŒ…ç®¡ç†å™¨å·²å®‰è£…
- [ ] Chrome æµè§ˆå™¨å·²å®‰è£…
- [ ] ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆå¦‚éœ€ä»£ç†è®¿é—®ï¼Œç¡®ä¿ä»£ç†åœ°å€æ­£ç¡®ï¼‰

### âœ… é¡¹ç›®æ£€æŸ¥
- [ ] é¡¹ç›®ç›®å½•å­˜åœ¨ `src/essay_manager.py`
- [ ] é¡¹ç›®æ ¹ç›®å½•æœ‰ `.claude/skills/crawl/` ç›®å½•
- [ ] MCP é…ç½®æ–‡ä»¶åŒ…å«æ­£ç¡®çš„ chrome-devtools é…ç½®

### âœ… MCP é…ç½®éªŒè¯
ç¡®ä¿ `.claude/mcp.json` åŒ…å«ä»¥ä¸‹é…ç½®ï¼š
```json
{
  "mcpServers": {
    "chrome-devtools": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "chrome-devtools-mcp@latest",
        "--browser-url=http://127.0.0.1:9222"
      ],
      "env": {}
    }
  }
}
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨æ”¹è¿›ç‰ˆç®¡ç†å™¨ï¼ˆæ¨èï¼‰

```python
# ç›´æ¥å¤åˆ¶ä½¿ç”¨
from output.improved_crawl_manager import extract_x_tweets

# æå–Elon Muskçš„æœ€æ–°5ç¯‡æ¨æ–‡
result = extract_x_tweets("elonmusk", 5)
print(result)
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨æ ‡å‡†æ¨¡æ¿

```python
# å¤åˆ¶ä»¥ä¸‹ä»£ç åˆ°ä½ çš„è„šæœ¬ä¸­
import os
import sys
import platform
import subprocess
import time
import json
import requests
from datetime import datetime

# 1. ç¡®ä¿outputç›®å½•å­˜åœ¨
os.makedirs('output', exist_ok=True)

# 2. å¯åŠ¨Chromeæµè§ˆå™¨
system = platform.system()
if system == "Windows":
    chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
    cmd = [chrome_path, "--remote-debugging-port=9222", "--proxy-server=http://127.0.0.1:1087"]
elif system == "Darwin":
    chrome_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    cmd = [chrome_path, "--remote-debugging-port=9222", "--proxy-server=http://127.0.0.1:1087"]

subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
time.sleep(3)

# 3. å¯åŠ¨APIæœåŠ¡
try:
    requests.get("http://localhost:8000/api/health", timeout=5)
except:
    subprocess.Popen(["uv", "run", "python", "essay_manager.py"], cwd="src")
    time.sleep(5)

# 4. ä½¿ç”¨MCPå·¥å…·è¿›è¡Œçˆ¬å–
# ... åœ¨è¿™é‡Œæ·»åŠ ä½ çš„çˆ¬å–é€»è¾‘
```

## ğŸ› ï¸ å¸¸ç”¨å‘½ä»¤

### æ£€æŸ¥Chromeæ˜¯å¦è¿è¡Œ
```bash
curl http://127.0.0.1:9222
```

### æ£€æŸ¥APIæœåŠ¡çŠ¶æ€
```bash
curl http://localhost:8000/api/health
```

### æ‰‹åŠ¨å¯åŠ¨APIæœåŠ¡
```bash
cd src && uv run python essay_manager.py
```

### æŸ¥çœ‹æ—¥å¿—
```bash
tail -f output/logs/crawl_log_$(date +%Y%m%d).txt
```

## ğŸ“Š æ”¯æŒçš„ç½‘ç«™å’ŒåŠŸèƒ½

### ğŸ¯ å®Œå…¨æ”¯æŒ
- **X/Twitter** (x.com) - æ¨æ–‡æå–
- **The Atlantic** (theatlantic.com) - æ–‡ç« æå–
- **Medium** (medium.com) - æ–‡ç« æå–

### ğŸ”§ éƒ¨åˆ†æ”¯æŒ
- LinkedIn (éœ€è¦ç™»å½•)
- Reddit (åŸºç¡€æå–)
- æ–°é—»ç½‘ç«™ (é€šç”¨æå–)

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡å¤„ç†
```python
# æ¨èï¼šæ‰¹é‡æå–å¤šä¸ªç”¨æˆ·
users = ["elonmusk", "sundarpichai", "satyanadella"]
results = {}
for user in users:
    results[user] = extract_x_tweets(user, 3)
```

### 2. é”™è¯¯é‡è¯•
```python
# æ¨èï¼šä½¿ç”¨æ”¹è¿›ç‰ˆç®¡ç†å™¨ï¼Œè‡ªåŠ¨åŒ…å«é‡è¯•æœºåˆ¶
manager = ImprovedCrawlManager({"max_retries": 5})
```

### 3. æ•°æ®éªŒè¯
```python
# æ¨èï¼šéªŒè¯æ•°æ®æ ¼å¼åå†å‘é€
if manager.validate_data_format(api_data):
    result = manager.send_to_api(api_data)
```

## ğŸš¨ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### Q1: Chromeå¯åŠ¨å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥Chromeå®‰è£…è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿9222ç«¯å£æœªè¢«å ç”¨
3. å°è¯•æ‰‹åŠ¨å¯åŠ¨Chromeå¹¶æ·»åŠ è°ƒè¯•å‚æ•°

### Q2: MCPè¿æ¥å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥`.claude/mcp.json`é…ç½®
2. ç¡®ä¿chrome-devtools-mcpåŒ…å·²å®‰è£…
3. é‡å¯Claude Code

### Q3: APIæœåŠ¡å¯åŠ¨å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥`src/essay_manager.py`æ˜¯å¦å­˜åœ¨
2. ç¡®ä¿uvå·²å®‰è£…
3. æ£€æŸ¥8000ç«¯å£æ˜¯å¦è¢«å ç”¨

### Q4: æ•°æ®å†™å…¥å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚
2. ç¡®ä¿APIæœåŠ¡æ­£åœ¨è¿è¡Œ
3. æŸ¥çœ‹APIæœåŠ¡æ—¥å¿—

## ğŸ“ æ–‡ä»¶ç»“æ„è¯´æ˜

```
output/
â”œâ”€â”€ logs/              # æ—¥å¿—æ–‡ä»¶
â”‚   â””â”€â”€ crawl_log_*.txt
â”œâ”€â”€ data/              # æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ *_raw_*.json   # åŸå§‹æå–æ•°æ®
â”‚   â””â”€â”€ api_response_*.json
â”œâ”€â”€ snapshots/         # é¡µé¢å¿«ç…§
â”‚   â””â”€â”€ *_page_*.txt
â””â”€â”€ reports/           # æ‰§è¡ŒæŠ¥å‘Š
    â””â”€â”€ *_report_*.md
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å®Œæ•´æ–‡æ¡£](SKILL.md)
- [Chrome DevTools MCP é…ç½®](how-to-crawl-with-chrome-dev-mcp.md)
- [æ”¹è¿›ç‰ˆç®¡ç†å™¨æºç ](../output/improved_crawl_manager.py)

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨æ”¹è¿›ç‰ˆç®¡ç†å™¨** - åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
2. **éªŒè¯æ•°æ®æ ¼å¼** - ç¡®ä¿URLå­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
3. **ä¿å­˜åŸå§‹æ•°æ®** - ä¾¿äºè°ƒè¯•å’Œé‡è¯•
4. **æŸ¥çœ‹æ‰§è¡ŒæŠ¥å‘Š** - äº†è§£ä»»åŠ¡æ‰§è¡Œè¯¦æƒ…
5. **å®šæœŸæ¸…ç†æ—¥å¿—** - é¿å…æ—¥å¿—æ–‡ä»¶è¿‡å¤§

---

ğŸ‰ **ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½ä½¿ç”¨ crawl æŠ€èƒ½äº†ï¼**