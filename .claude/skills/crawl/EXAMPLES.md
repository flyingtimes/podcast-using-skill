# ğŸ“š Crawl æŠ€èƒ½ä½¿ç”¨ç¤ºä¾‹

## ğŸ¯ ç¤ºä¾‹ç›®å½•

| ç¤ºä¾‹ | æè¿° | å¤æ‚åº¦ |
|------|------|--------|
| [ç¤ºä¾‹1](#ç¤ºä¾‹1-æå–xtwitteræ¨æ–‡) | æå–X/Twitteræ¨æ–‡ | â­â­ |
| [ç¤ºä¾‹2](#ç¤ºä¾‹2-æå–the-atlanticæ–‡ç« ) | æå–The Atlanticæ–‡ç«  | â­ |
| [ç¤ºä¾‹3](#ç¤ºä¾‹3-æ‰¹é‡å¤„ç†mediumæ–‡ç« ) | æ‰¹é‡å¤„ç†Mediumæ–‡ç«  | â­â­â­ |
| [ç¤ºä¾‹4](#ç¤ºä¾‹4-ä¸æ•°æ®åº“é›†æˆ) | ä¸æ•°æ®åº“é›†æˆ | â­â­â­ |
| [ç¤ºä¾‹5](#ç¤ºä¾‹5-è‡ªå®šä¹‰çˆ¬è™«) | è‡ªå®šä¹‰çˆ¬è™« | â­â­â­â­ |

---

## ç¤ºä¾‹1: æå–X/Twitteræ¨æ–‡

### åŸºç¡€ç‰ˆæœ¬
```python
from .crawl_manager import extract_x_tweets

# æå–Elon Muskçš„æœ€æ–°5ç¯‡æ¨æ–‡
result = extract_x_tweets("elonmusk", 5)
print(result)
```

### é«˜çº§ç‰ˆæœ¬ï¼ˆè‡ªå®šä¹‰é…ç½®ï¼‰
```python
from .crawl_manager import ImprovedCrawlManager

# è‡ªå®šä¹‰é…ç½®
config = {
    "output_dir": "output",
    "api_base_url": "http://localhost:8000",
    "max_retries": 5,
    "timeout": 60
}

manager = ImprovedCrawlManager(config)

# æå–å¤šä¸ªç”¨æˆ·çš„æ¨æ–‡
users = ["elonmusk", "sundarpichai", "satyanadella"]
results = {}

for user in users:
    try:
        result = extract_x_tweets(user, 3, config)
        results[user] = result
        print(f"{user}: {result.get('tweets_extracted', 0)} æ¡æ¨æ–‡")
    except Exception as e:
        print(f"{user} æå–å¤±è´¥: {e}")
```

### æ‰‹åŠ¨ç‰ˆæœ¬ï¼ˆå®Œæ•´æ§åˆ¶ï¼‰
```python
import os
import platform
import subprocess
import time
import json
import requests
from datetime import datetime

def manual_extract_tweets(username: str, count: int = 5):
    """æ‰‹åŠ¨æå–æ¨æ–‡çš„å®Œæ•´æµç¨‹"""

    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # 2. å¯åŠ¨Chromeæµè§ˆå™¨
    system = platform.system()
    if system == "Windows":
        chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
        cmd = [chrome_path, "--remote-debugging-port=9222",
               "--proxy-server=http://127.0.0.1:1087"]
    elif system == "Darwin":
        chrome_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
        cmd = [chrome_path, "--remote-debugging-port=9222",
               "--proxy-server=http://127.0.0.1:1087"]

    subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(3)

    # 3. å¯åŠ¨APIæœåŠ¡
    try:
        requests.get("http://localhost:8000/api/health", timeout=5)
    except:
        subprocess.Popen(["uv", "run", "python", "essay_manager.py"],
                         cwd="src")
        time.sleep(5)

    # 4. ä½¿ç”¨MCPå·¥å…·æå–
    mcp__chrome-devtools__new_page(url=f"https://x.com/{username}", timeout=15000)
    time.sleep(5)

    # 5. æå–æ¨æ–‡
    tweets_data = mcp__chrome-devtools__evaluate_script(function=f"""
        () => {{
            const tweets = [];
            const tweetElements = document.querySelectorAll('article');
            const maxTweets = Math.min({count}, tweetElements.length);

            for (let i = 0; i < maxTweets; i++) {{
                const tweet = tweetElements[i];
                const timeLink = tweet.querySelector('a[href*="/status/"]');

                if (timeLink) {{
                    const tweetUrl = timeLink.href;
                    const textElement = tweet.querySelector('[data-testid="tweetText"]');
                    const tweetText = textElement ? textElement.innerText.trim() : '';

                    if (tweetText && tweetText.length > 10) {{
                        tweets.push({{
                            title: tweetText.length > 50 ? tweetText.substring(0, 50) + '...' : tweetText,
                            subtitle: '@{username}çš„æ¨æ–‡',
                            author: '{username}',
                            url: tweetUrl,
                            content: tweetText,
                            entry_time: new Date().toISOString().slice(0, 19).replace('T', ' ')
                        }});
                    }}
                }}
            }}

            return {{ success: true, tweets: tweets }};
        }}
    """)

    # 6. å‘é€åˆ°API
    api_data = {"essays": tweets_data.get("tweets", [])}
    response = requests.post("http://localhost:8000/api/essays",
                           json=api_data,
                           headers={"Content-Type": "application/json"})

    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = manual_extract_tweets("elonmusk", 5)
print(result)
```

---

## ç¤ºä¾‹2: æå–The Atlanticæ–‡ç« 

### åŸºç¡€ç‰ˆæœ¬
```python
import sys
import os
sys.path.append('C:\\Users\\13802\\code\\podcast-using-skill\\src')
from content_extractor import extract_content_from_url

def extract_atlantic_article():
    """æå–The Atlanticæ–‡ç« """
    os.makedirs('output', exist_ok=True)

    url = "https://www.theatlantic.com/newsletters/2025/11/baseball-gambling-charges-mlb-cleveland-guardians/684896/"

    # ä½¿ç”¨å†…å®¹æå–å™¨
    result = extract_content_from_url(url)

    if result['success']:
        print(f"æ ‡é¢˜: {result['title']}")
        print(f"ç½‘ç«™ç±»å‹: {result['site_type']}")
        print(f"å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")

        # ä¿å­˜åˆ°outputç›®å½•
        with open('output/atlantic_article.txt', 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {result['title']}\\n\\n")
            f.write(result['content'])

        return result
    else:
        print(f"æå–å¤±è´¥: {result['error']}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
result = extract_atlantic_article()
```

### é«˜çº§ç‰ˆæœ¬ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
```python
def batch_extract_atlantic_articles(urls: list):
    """æ‰¹é‡æå–The Atlanticæ–‡ç« """
    import sys
    import os
    import json
    import requests
    from datetime import datetime

    sys.path.append('C:\\Users\\13802\\code\\podcast-using-skill\\src')
    from content_extractor import batch_extract_content

    os.makedirs('output', exist_ok=True)

    # æ‰¹é‡æå–
    results = batch_extract_content(urls)
    successful_extractions = [r for r in results if r['success']]

    print(f"æˆåŠŸæå–: {len(successful_extractions)}/{len(urls)} ç¯‡")

    # å‡†å¤‡APIæ•°æ®
    api_data = {"essays": []}

    for result in successful_extractions:
        api_data["essays"].append({
            "title": result['title'],
            "url": result['url'],
            "content": result['content'],
            "entry_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "author": "The Atlantic",
            "subtitle": result.get('subtitle', '')
        })

    # ä¿å­˜åŸå§‹æ•°æ®
    with open('output/atlantic_articles_raw.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # å‘é€åˆ°API
    try:
        response = requests.post(
            "http://localhost:8000/api/essays",
            json=api_data,
            headers={"Content-Type": "application/json"}
        )

        if response.status_code == 200:
            api_result = response.json()
            print(f"æˆåŠŸå†™å…¥ {api_result.get('success_count', 0)} ç¯‡æ–‡ç« ")
            return api_result
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {e}")

    return None

# ä½¿ç”¨ç¤ºä¾‹
urls = [
    "https://www.theatlantic.com/newsletters/2025/11/baseball-gambling-charges-mlb-cleveland-guardians/684896/",
    "https://www.theatlantic.com/health/2025/11/supplement-patches-wellness/684893/"
]

result = batch_extract_atlantic_articles(urls)
```

---

## ç¤ºä¾‹3: æ‰¹é‡å¤„ç†Mediumæ–‡ç« 

```python
def batch_process_medium_articles():
    """æ‰¹é‡å¤„ç†Mediumæ–‡ç« """
    import sys
    import os
    import json

    sys.path.append('C:\\Users\\13802\\code\\podcast-using-skill\\src')
    from content_extractor import batch_extract_content

    os.makedirs('output', exist_ok=True)

    urls = [
        "https://medium.com/@username/article1",
        "https://medium.com/@username/article2",
        "https://medium.com/@username/article3"
    ]

    # æ‰¹é‡æå–
    results = batch_extract_content(urls)
    successful_extractions = [r for r in results if r['success']]

    print(f"æˆåŠŸæå–: {len(successful_extractions)}/{len(urls)} ç¯‡")

    # ä¿å­˜ç»“æœåˆ°outputç›®å½•
    for result in successful_extractions:
        filename = f"output/medium_{result['title'][:20].replace(' ', '_')}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {result['title']}\\n")
            f.write(f"URL: {result['url']}\\n")
            f.write(f"ç½‘ç«™: {result['site_type']}\\n\\n")
            f.write(result['content'])

    return successful_extractions

# ä½¿ç”¨ç¤ºä¾‹
results = batch_process_medium_articles()
```

---

## ç¤ºä¾‹4: ä¸æ•°æ®åº“é›†æˆ

```python
def extract_and_store_to_database(urls):
    """æå–æ–‡ç« å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    import sys
    import os
    import json
    import requests
    from datetime import datetime

    sys.path.append('C:\\Users\\13802\\code\\podcast-using-skill\\src')
    from content_extractor import batch_extract_content

    os.makedirs('output', exist_ok=True)

    # æ‰¹é‡æå–
    results = batch_extract_content(urls)

    # å‡†å¤‡APIæ•°æ®
    api_data = {
        "essays": []
    }

    for result in results:
        if result['success']:
            api_data["essays"].append({
                "title": result['title'],
                "url": result['url'],
                "content": result['content'],
                "entry_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "author": result.get('author', ''),
                "subtitle": result.get('subtitle', '')
            })

    # ä¿å­˜æå–æ•°æ®åˆ°outputç›®å½•ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
    with open('output/extracted_articles_data.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)

    # å‘é€åˆ°API
    try:
        response = requests.post(
            "http://localhost:8000/api/essays",
            json=api_data,
            headers={"Content-Type": "application/json"}
        )

        if response.status_code == 200:
            api_result = response.json()
            print(f"æˆåŠŸå­˜å‚¨ {api_result.get('success_count', 0)} ç¯‡æ–‡ç« ")

            # ä¿å­˜APIå“åº”åˆ°outputç›®å½•
            with open('output/api_response.json', 'w', encoding='utf-8') as f:
                json.dump(api_result, f, ensure_ascii=False, indent=2)

            return api_result
        else:
            print(f"å­˜å‚¨å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        print(f"å‘é€APIè¯·æ±‚æ—¶å‡ºé”™: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
urls = [
    "https://www.theatlantic.com/newsletters/2025/11/baseball-gambling-charges-mlb-cleveland-guardians/684896/",
    "https://medium.com/@username/article1"
]

result = extract_and_store_to_database(urls)
```

---

## ç¤ºä¾‹5: è‡ªå®šä¹‰çˆ¬è™«

```python
class CustomCrawler:
    """è‡ªå®šä¹‰çˆ¬è™«ç±»"""

    def __init__(self):
        self.output_dir = "output"
        os.makedirs(self.output_dir, exist_ok=True)

    def crawl_website(self, url: str, selectors: dict):
        """
        é€šç”¨ç½‘ç«™çˆ¬å–

        Args:
            url: ç›®æ ‡URL
            selectors: CSSé€‰æ‹©å™¨å­—å…¸
                {
                    'title': 'h1.title',
                    'content': 'div.content',
                    'author': 'span.author',
                    'date': 'time.date'
                }
        """
        try:
            # å¯¼èˆªåˆ°é¡µé¢
            mcp__chrome-devtools__navigate_page(
                type="url",
                url=url,
                ignoreCache=False,
                timeout=10000
            )

            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)

            # æå–æ•°æ®
            extracted_data = mcp__chrome-devtools__evaluate_script(function=f"""
                (selectors) => {{
                    const data = {{}};

                    for (const [key, selector] of Object.entries(selectors)) {{
                        const element = document.querySelector(selector);
                        if (element) {{
                            data[key] = element.innerText.trim();
                        }}
                    }}

                    return data;
                }}
            """, args=[{"selectors": selectors}])

            # æ·»åŠ å…ƒæ•°æ®
            extracted_data['url'] = url
            extracted_data['crawl_time'] = datetime.now().isoformat()

            # ä¿å­˜æ•°æ®
            filename = f"{self.output_dir}/custom_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=2)

            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return extracted_data

        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥: {e}")
            return None

# ä½¿ç”¨ç¤ºä¾‹
def custom_crawl_example():
    """è‡ªå®šä¹‰çˆ¬å–ç¤ºä¾‹"""
    crawler = CustomCrawler()

    # å®šä¹‰é€‰æ‹©å™¨
    selectors = {
        'title': 'h1',
        'content': 'article p',
        'author': '.author-name',
        'date': 'time'
    }

    # çˆ¬å–ç½‘ç«™
    result = crawler.crawl_website("https://example-news.com/article/123", selectors)
    print(result)

# è¿è¡Œç¤ºä¾‹
custom_crawl_example()
```

---

## ğŸ”§ å¸¸ç”¨å·¥å…·å‡½æ•°

```python
def save_data(data, filename_prefix: str):
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"output/{filename_prefix}_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
    return filename

def validate_url(url: str) -> bool:
    """éªŒè¯URLæ ¼å¼"""
    return url.startswith(('http://', 'https://'))

def wait_for_element(selector: str, timeout: int = 10):
    """ç­‰å¾…å…ƒç´ å‡ºç°"""
    mcp__chrome-devtools__wait_for(
        text=selector,
        timeout=timeout * 1000
    )

def take_page_snapshot(filename: str = None):
    """ä¿å­˜é¡µé¢å¿«ç…§"""
    if not filename:
        filename = f"page_snapshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    mcp__chrome-devtools__take_snapshot(
        verbose=True,
        filePath=f"output/snapshots/{filename}"
    )
```

## ğŸš¨ é”™è¯¯å¤„ç†ç¤ºä¾‹

```python
def safe_extract_content(url, max_retries=3):
    """å®‰å…¨çš„å†…å®¹æå–ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            result = extract_content_from_url(url)
            if result['success']:
                return result
            else:
                print(f"å°è¯• {attempt + 1} å¤±è´¥: {result.get('error')}")
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
        except Exception as e:
            print(f"å°è¯• {attempt + 1} å¼‚å¸¸: {e}")
            time.sleep(2)

    return {'success': False, 'error': f'ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥'}

# ä½¿ç”¨ç¤ºä¾‹
result = safe_extract_content("https://example.com/article")
```

---

## ğŸ“š æ›´å¤šèµ„æº

- [æœ€ä½³å®è·µæŒ‡å—](BEST_PRACTICES.md)
- [å¿«é€Ÿå¯åŠ¨æŒ‡å—](QUICK_START.md)
- [æ ¸å¿ƒç®¡ç†å™¨ä»£ç ](crawl_manager.py)